{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  new_loss = loss + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(new_loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_vals = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_vals = []\n",
    "\n",
    "for regul_val in regul_vals:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul_val}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      accuracy_vals.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEMCAYAAADEXsFmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12173f048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_vals, accuracy_vals)\n",
    "plt.grid(True)\n",
    "plt.title('Test Accuracy by Regularization (Logistic Regression)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  new_loss = loss + beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(new_loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_vals = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_vals = []\n",
    "\n",
    "for regul_val in regul_vals:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul_val}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      accuracy_vals.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEMCAYAAADDMN02AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd8FHX6wPHPkx5CDxBI6ALSi4ZiQbGjoiKe7bAX9Cw/\nvfPOdud5p2c7r3ieZ0HFAioWRGkqiAYrLdI7SA8QWgjp7fn9MYMucZNskk0myT7v12tfuzsz35ln\ndr/7zOz3O0VUFWOMMaEjzOsAjDHG1C5L/MYYE2Is8RtjTIixxG+MMSHGEr8xxoQYS/zGGBNiLPGb\nShGRkSKy0es4qktE5ovIVdUov0lETghyTNEikiUiicGcr8/8/y0it9bEvE3FRGSyiPypjHH/E5Hr\nayuWBpX43R/NkUeJiOT6vB9bjfkGlCREpLm7zKlVXVYocX8I+e73c0BEPhWRbl7HFQhVPUZVv6/O\nPErXK1XNV9XGqppW/Qh/sawk4FfABPd9nIh8KCJbRURFZFgF5evkBt/9DLNFpK3PsFEisjbA8k+K\nyCs1F2HAngYeFpHw2lhYg0r87o+msao2BrYBF/gMe6sWQrgcyAHOE5H4WljeT0QkojaXF0SPut9X\ne+Ag8JLH8ZSrHn/ONwAfqWqB+16BFOBKnM+9zisnKeYBD9ZmLJVVUUJX1S3AduDc2oinQSX+iohI\nuIg8JCI/isg+EXlLRJq74+LcPdADIpIhIgtEpIWI/BMYDLzi7pn+s5xFXAs8A2zC+UH5LruziHzs\nLnef73xE5DYRWSsih0VkhYj0E5EYd0+svc90P/1VPLIH5q7PHuAFEWktIp+IyF53PT4WkXY+5VuJ\nyJsisltEDorIu+7wjSJyls90MSJySER6lfNZ/tVdxmYRudQdNlxEtouI+Ez3axFZUO4XA6hqDvA+\nMLDUcm4RkXXusma6e65Hxp0vIhvc7+sZ3z3o0ntyItJTRIrKWJeeIpLiLmOviLwhIk18xu8Wkd+L\nyCog02fYyW6d8v2nme1+b23L+z781avS37mItBSRt93ym0Xk3iOfrYjcKiJzReRZd/03iciZ5XzE\n5wLzfD9vVX1WVb8DSir6fsojIheLyDIRyRSRbSLyoM+4uSJyc6np14nIue7rviLyhVsf14jIaJ/p\nJrvrN1tEsoGymtaeAa4XkY5lxNfB57f3o7jNXe6yfgdc634HC0XkXBFZ5FP2axH52uf9IhEZ6b7u\n547PEJHlR9YpkNhFpJmIfCMiT/sMTgHOL2Mdg0tVG+QD2AKcWWrYfcDXQCIQA7wOvOaOuwv4AIgF\nInB+lHHuuPnAVRUsrwfOD+gY4I/AIp9xkcAa4EmgkbuME91xVwNbgUGAAMfi7P3G4OyVtfeZz2Tg\nT+7rkUAR8AgQ5c4zAbjIfd0M+BiY7FN+LjARaO6WOcUd/mfgDZ/pLveNv9R6HlnuE+48zsT5l9PF\njX8TcJrP9J8At5cxL9/1aQK8BywoFcca97ONBP4GfOmOawdkAaPccfcChUe+J/ezfsVnXj2BIp/3\n832m7Qmc7q5PW3fckz7T7gYWufUm1mfYyX7W6V/A50B4AN/HUfWq9Hfufh7vA42BbsBmYKw77lZ3\nfa9xl/VbYEs59fMw0K+McfuAYRXU75HAxjLGnQH0wdmRPA44AIx0x10DzPOZdiiwy425qft6rPt+\nsFu2m0/9OOCWCQOi/Sx7PnAV8PyR79utE2vd1+HACpzffpRbl7YBp5ZRT5oC+e5zjBvfLvd1E5x/\nF03c99uAe9z6dw5OfexSVuzusD8BbYAluHXfZ9m/Br6rlfxYGwvx4oH/xL8ZOMnnfRecpCXAbTh7\nRH3LqlwVLO9vwHyf+SrQy31/GrATCPNTbh5wi5/hgST+bCCynJiGAbt8YioAmviZrjNwiJ+T2gzg\n/8qY50i38sf4DJsG/MF9/TDwqvs6wf18W5Uxr8lALpDhrutGoLfP+C9xE537PhIn2SUA43A3Au64\nMCCdKiR+P3FdAXzv83438OtS0/wi8eMkuY1Ay4q+D38x+H7nOImiGOjqM/4u4FP39a3ASp9xLd2y\nzf0sN9wd17mMuKqV+P1M+yLwhPs6DudfUkf3/XPAv9zX1wJzSpV9A7jPp36Mr2BZRxJ/Is7GrTtH\nJ/5TgQ2lyvwVeMFfPXGHLQLOA0a4dXua+/pcYKE7zVk4O2ziU24qcH9ZsR8ZBqwF7vSzLhcAqwP5\njKv7CJmmHvcvcgdglvvXLANnqxsGxAOv4iThD0Rkh4g8LgF2tLjzvhp4C0BVNwPf41Rs3OVuVlV/\nf6k74OwlV8VuVS30iaOJiExw/25nArOBVj7LSVfVw6Vnok774hJgtIi0xtn7nVzOcveqap7P+604\nPzyAN4ExIhKD09w1R1X3lTOvx1S1OdAVJ9H5du52Al70+b724vzbaO8ub7vPOpTgbFwrTUQSReR9\nEdnpfm6v8PPndsR2P0V95zEU+CdwkaoecIeV931UpC1O3dzmM2wrkOTzfrfP6xz3uXHpGalqMU5S\nbFJ6XBnr0sOn6aq87+7I9CeJyDy3SeoQcB3ueqpqNvAhMFZEInH+xU10i3YCTjny/brf8SU4/+aO\nKPdz91nHNJyk+pdSozoBnUst43c4n29Z5uEk+lPc1yk4G5BT+bm5LBHYpm7GdpX+fvzFfhHORniC\nn3FNcHaCalzIJH73C9oJnK6qzX0eMaq6T50jKv6sqj1xvvBLcfb8wPmiynMa0BH4i9v2uxsYAFwl\nImE4FaCz+7q07TjNQ6UV4OzdNvIZVrqylo7rfpykOFhVmwJn4/ybObKcNiLyi8TgegNnz+kK4AtV\nTS9jOoBWbmI/oiOQBj9t9Jbj7L1czc8/8nK55X4P/FdEonxivq7U9xWrqqk4f799+z/COPpHl035\nn52vp93p+7qf2038/Ln9FGJZhcU5/HIKcJOqrvIZVd73Ue48cZJ6Cc5ne0RHqrhxw/lOegQyoaqu\n158PighkQ/Ue8C7QQVWb4TSh+q7nGzjNOSOBPaq6xB2+HZhd6vttrKp3+4YTSMyuJ3H29vv6DNuO\ns/fvu4wmqnpxOfMvnfjn8cvEn8bR3w388vvxN+/ngO+AaSISW2pcL2BZ2asXPCGT+F0vAk+KSAcA\nEWkjIhe4r88Ukd5uAsnE2bM8soe+B2ePtCzX4jSP9MHpnByIk/hb4rR/foOzx/WoiDQSkVgROdEt\n+wpwv4gMEEcPEWnv7sGuwNlTCheRCym7c+uIJjh7fhki0gqnPRH4KbF+BTzndixFicgpPmU/AE4G\nfoOz116eSOAhdx6n4/ztneIz/k3gIZzPbHoF8/qJqk7H+eyPHM/8IvAnETkWQJzO9kvccdOAoSJy\nnjhH2vwOaOEzu6XAaSKSJCItcNp4y9IEp3020+0g/F2gMbsbqQ+Bl1T1Yz/z9ft9uMqsV6qaj9N0\n8Lg4Bx4cg9PUMynQ2EqZhZO4fGOP9tmAR5XamPsj4nRA+z4E51/GflXNc+v1paXKpeB8Fo9xdN36\nCBgkIpeLSKRbn4aJSEAbqNJUdS/wLPAHn8HfuIHf7cYbISL9ReQ4d/weoIu7Hkd8jfP77YvzT3gJ\nTlIedGR+7jRh7nwjxDk44mycjWC5YQI342w4PhKRaJ9xp+L0idW4UEv8f8fpePtCRA7jbHmPVIAk\nnM63w8BKnB/Ku+64fwPXiHPkwd99Z+juQV8CPKuqu30eG3GaS651m2POw6lMO3D+vl8MoKoTcToE\nP3CX/QFO5yvAHTh/jQ8Co3E2LuX5B85f7P04FXRWqfFX4iTtDTh7lL85MsJtApqO8xd2WgXL2YKz\nYdyN85f1elX90Wf8+zhNNu+5Cawy/oGzIYxU1Xdw9pA+dJtKluJsZFDVXe76PIvTRt0eZ0N5ZHkz\ncT6v1TjtwB+Vs8w/42z0DuEk2ynlTFtaV5wOvPvk6KN72lDx91FmvXLd4j5vBb7A2Umo6mHJr+M0\n5UX5DNuK08cSj7Mnmys+x8P70dWd3veRhNPf8A/3N3Uvzvf/E/ff9kScHaO3fIYfxOkUvR7nH1wa\nTl9ZZBXXEZzP/Kck7vPbOxFnffcCL/Bzk9hknH+GB0TkO7dMBk69WaKqxe5OWCqwxh2H29Q5Cufc\niP04v+HLS/0O/HLndx1Os84Ud4PXCadZamY11j1gcnQTlQllIvI40EZVb6rmfI60TV+hqt9UNH0w\nuHv9u3HO3ajWiVUNlYj8C1ivqi96sOxxwGWqWt4hpyFLRP4HpKqqv7b/oKuvJ6OYIBOnU/c6nH8W\n1XUlkFnTSV+c46a/w9nL/yNOs0pqTS6zPlPVgJuwgklE4nD+XT7hxfLrA1W9vTaXF2pNPcYPEbkD\np/nmfVVdWM15zcc5uuWOIIRWkVNwDtFNx+lLuVh/PjPV1AFu31Q6zmGuH3gcjnFZU48xxoQY2+M3\nxpgQY4nfGGNCTJ3s3G3VqpV27ty5SmWzs7OJi4sLbkDGBMjqn/FKamrqPlVtHci0dTLxd+7cmcWL\nF1epbEpKCiNGjAhuQMYEyOqf8YqIbA10WmvqMcaYEGOJ3xhjQowlfmOMCTGW+I0xJsRY4jfGmBBj\nid8YY0JMnTyc05iGpqREKSguIb+whLyiYvILS8gvKia/yH12hzeLjaRXu6Y0irKfpqk5VruMCZKX\n5m1i0re5RKam/JzYC0vILyqhoNjfXTf9E4EureLom9iMPolN6eM+t4iLqriwMQGwxG9MEKxKO8ST\nn66lc5MwerdrSnREODGRYURHhBMdGUZ0hPs6IoyYSOc5OvLoYVERYew7nM+qtExWph0idetBpi1L\n+2kZic1i6JN09MagXbMYjr55lDEVs8RvTDWpKo/NXEPz2Eh+PziS8886ruJC5Tizd8JPrw9mF7Aq\nLZNVaYd+ev58zR6OXFS3ZVwUfRKb0tvdGJx0TDzxjaPLmLMxDkv8xlTTl+vS+W7Tfv5yQW/iCgM+\naz4gLeKiOLl7K07u/vM9z7Pzi1i7O9PZEOzMZNWuQ0z4ZjOFxUqrxlFMumkoPds2DWocpmGxxG9M\nNRQVl/D4rLV0aRXH2GGd+Pbr4CZ+f+KiIzi+U0uO79Typ2EFRSUs35HB7W//wBXj5zPpxqH0TWpW\n47GY+skO5zSmGt5ZtJ2N6Vncf25PIsO9+zlFRYSR3Lkl791yAnFREVz58nxStx70LB5Tt1niN6aK\nDucV8syc9Qzp0pKzfdrlvdQpPo73bz2B+Lgorn51Ad9v2u91SKYOssRvTBW9kLKJ/dkF/On8XnXq\nyJrE5rG8d8sJJDWP5brXFjJv/V6vQzJ1jCV+Y6pgZ0Yur36zmdEDE+nfvrnX4fxCm6YxTB43jGNa\nN+bmNxYze9Vur0MydUhAiV9E7hKRlSKySkTudoc9KiLLRWSpiMwWkcQyyl4rIhvcx7XBDN4Yr/zj\ns3Uo8IeRPb0OpUzxjaN55+Zh9E5sym1v/cB0n3MCTGirMPGLSF/gZmAIMAAYJSLdgKdVtb+qDgRm\nAH/2U7Yl8DAw1C3/sIi0CGL8xtS65TsymLpkJzee3IWk5rFeh1OuZo0imXTTUI7r1IK7Ji/hg9Qd\nXodk6oBA9vh7AQtUNUdVi4B5wBhVzfSZJg5QP2XPAeao6gFVPQjMAUZWN2hjvHLkZK34uChuG3GM\n1+EEpHF0BG9cP4STurXi9+8vY+L8mj/k1NRtgRzHvxJ4TETigVzgPGAxgIg8BlwDHAJO81M2Cdju\n836HO+wXRGQcMA4gISGBlJSUwNaglKysrCqXNaYiP+wpYsHmfK7pHUXq/G9/Mb4u17+rOyuZGeE8\n9NFKVq9dzzmdI70OyXhEVP3tqJeaSORG4DYgG1gF5Kvq3T7jHwBiVPXhUuV+7w7/m/v+ISBXVf9R\n3vKSk5PVbrZu6prC4hLO+fdXiMBnd59ChJ/j9ut6/SsoKuG37y5l5opd/P7sHtxxenevQzJBIiKp\nqpocyLQBde6q6quqeryqngIcBNaXmuQt4BI/RXcCHXzet3eHGVPvvL1gGz/uy+bB83r5Tfr1QVRE\nGP+5YiBjBiXxj9nrefqztQSy82caloAu2SAibVQ1XUQ6AmOAYSLSXVU3uJNcBKz1U/Qz4HGfDt2z\ngQeqG7Qxte1QbiHPfL6eE4+J5/SebbwOp1oiwsP4x6UDiI4M539fbiK3oISHRtWtcxFMzQr0Wj1T\n3Db+QuB2Vc0QkVdF5FigBNgK3AogIsnArap6k6oeEJFHgUXufB5R1QNBXgdjatzzKRvJyC3kwfMa\nRoIMCxMev7gv0RFhTPh2M3lFxfztor6EhdX/dTMVCyjxq+pwP8P8Ne2gqouBm3zeTwAmVDVAY7y2\n/UAOr32zhTGD2jeoC5+JCA9f0JvYqHBeSNlEXmExf7+kf71txjKBs6tzGlOBv3+2jrAw+P05PbwO\nJehEhHvPOZZGkeH8c8569mTm8X+nd2dIl5YN4p+N8c8SvzHlWLLtINOXpXHn6d1o16xun6xVVSLC\nnWd0p3lcFP+avY7Lx8+nf/tm3HhyF87r187Tq46ammHfqDFlOHKyVqvG0dxyav04Was6rh7Wie/u\nP4O/je5LVl4Rd01eyoinU3j5qx/JzCv0OjwTRJb4jSnDpyt3s3jrQX53Vg8aR4fGn+PYqHCuGtaJ\nz393Ki9fk0xSi1gem7WGE5/4gr/NWM3OjFyvQzRBEBq12ZhKKigq4clP19IjoTGXJbf3OpxaFxYm\nnNU7gbN6J7B8RwavfL2Z177bwmvfbeHcvm25eXhXBnSoe1clNYGxxG+MHxPnb2Xr/hxev35wyB/l\n0r99c569chD3nduT17/dzOSF25mxfBdDOrfkpuFdOLNXgh0GWs+Edo02xo9DOYU8O3cDw7u34tQe\nrb0Op85Iah7LH8/vzXcPnM6fzu/Fzoxcxk1M5Yx/zWPi/K3kFhR7HaIJkCV+Y0r57xcbyMxrOCdr\nBVuTmEhuGt6VeX8YwX+vHETTmAge+mglJzw5lwnfbPY6PBMAa+oxxsfW/dm88f0WLj2+Pb3aNfU6\nnDotIjyMCwYkMqp/OxZtOchzX27kkRmryc4v4s4z7OJvdZnt8Rvj4++friMiLIx7zj7W61DqDRFh\nSJeWvH7dYMYMSuKfc9bz0rxNXodlymF7/Ma4UrceYOaKXdx9ZncSmsZ4HU69ExYmPH3pAApLlCc+\nWUtkeBg3nNzF67CMH5b4jQGy84v449SVtGkSzbhTunodTr0VHib867IBFBWX8MiM1USGC1ef0Nnr\nsEwp1tRjQl5RcQl3vrOEDelZPPWr/jSKsv2h6ogMD+M/VwzizF5teOjjVUxeuM3rkEwplvhNSFNV\n/jJ9FV+sTeeRi/pw2rH1+1r7dUVURBj/G3scp/ZozQNTVzDFbvJep1jiNyHtla83M2n+Nm45tStj\nh3byOpwGJToinJeuPp6TjmnFHz5YxrRlaV6HZFyW+E3I+mTFLh7/ZA3n92vHfef09DqcBikmMpyX\nr0lmcOeW/PbdpXyyYpfXIRks8ZsQ9cO2g9z97lIGdWjOPy8bYJccqEGxUeFMuG4wAzs05853ljBn\n9R6vQwp5ASV+EblLRFaKyCoRudsd9rSIrBWR5SIyVUT8XrFJRLaIyAoRWSoii4MZvDFVsXV/Nje/\nsZi2zWJ4+ZpkYiLDvQ6pwYuLjuC16wfTJ6kZt72Vypfr0r0OKaRVmPhFpC9wMzAEGACMEpFuwByg\nr6r2B9ZT/k3UT1PVgaqaHISYjamyjJwCrn9tEcWqvHbdYOIbR3sdUshoGhPJm9cP4di2TbhlYirf\nbNjndUghK5A9/l7AAlXNUdUiYB4wRlVnu+8B5gOhd+1aU6/kFxUz7s1UdhzM5eVrkunaurHXIYWc\nZo0imXjDULq2iuOmNxfx/ab9XocUkkRVy59ApBfwMXACkAvMBRar6p0+00wH3lXVSX7KbwYOAgq8\npKrjy1jOOGAcQEJCwvGTJ0+u0gplZWXRuLH9oM3RSlQZvzyf+buKuXVANMPa1cyx+lb/ApOZrzy5\nKJf9ucrvk2Po3sKa26rrtNNOSw20VaXCxA8gIjcCtwHZwCogX1WPtPX/EUjG+Rfwi5mJSJKq7hSR\nNjjNQ3eq6lflLS85OVkXL65ad0BKSgojRoyoUlnTcP3js3U89+VG7h15LLeN6FZjy7H6F7j0w3lc\n8dJ80g/nM/HGIQzq2MLrkOo1EQk48QfUuauqr6rq8ap6Cs7e+3p3QdcBo4Cx/pK+W3an+5wOTMXp\nKzCm1ry3aDvPfbmRKwZ34DchcO/c+qJNkxjevnkY8Y2juGbCQlbsOOR1SCEj0KN62rjPHYExwNsi\nMhK4F7hQVXPKKBcnIk2OvAbOBlYGI3BjAvH1hr08OHUFw7u34tHRfe36+nVM22ZO8m8aE8lVry5g\ndVqm1yGFhECP458iIquB6cDtqpoBPAc0Aea4h2q+CCAiiSIyyy2XAHwjIsuAhcBMVf00uKtgjH9r\nd2dy26Qf6NamMc+PPY7IEL+FYl2V1DyWd24eRqOocC554Tte/WYzxSUVN0Gbqguoh0tVh/sZ5reh\nVFXTgPPc1z/iHAJqTK3ak5nHDa8tolG0c/JQk5hIr0My5egY34gPbzuRBz9cwaMzVjNtWRpPXdKP\nnm3tZjg1wXaBTIOTnV/EDa8v4lBuIROuG0xi81ivQzIBaNcslgnXDebZKwex40AOo579hn98to68\nQruXb7BZ4jcNypFLLK/Zlclzvz6OPonNvA7JVIKIcOGARD7/3alcODCR577cyHnPfs3CzQe8Dq1B\nscRvGoyjL7Hcl9N62iWW66sWcVH867KBvHnDEAqKSrjspe/549QVZOYVeh1ag2CJ3zQYr327xbnE\n8ilduWqYXWK5ITilR2tm//YUbjq5C+8s3MZZ/5rH7FW7vQ6r3rPEbxqEgqISnvl8Paf2aM19I+0S\nyw1Jo6gI/jSqN1NvO4kWjaIYNzGV295KJf1wnteh1VuW+E2D8M3GvWTmFXHNCZ3sEssN1IAOzZl+\n58n84Zxj+XxNOmf+cx7vLdpOIFcfMEezxG8ahBnLdtE0JoLh3Vt7HYqpQZHhYdx+Wjc+uWs4Pds1\n5d4py/n1ywvYsi/b69DqFUv8pt7LKyxm9uo9nNOnLVERVqVDwTGtGzP55mE8fnE/Vu48xDnPfMWL\n8zZRVFzidWj1gv1KTL03b/1esvKLGDUg0etQTC0KCxN+PbQjn99zKqf2aM2Tn6zl2tcWUmJn/VbI\nEr+p92Yu30WLRpGceEy816EYDyQ0jWH8Ncn85YLefLtxPx+k7vA6pDrPEr+p13ILivl8zR5G9m1n\n1+IJcdee2JnBnVvw5Kdrycgp8DqcOs1+KaZe+3JdOjkFxYzq387rUIzHRIS/XtiXjJwC/jl7vdfh\n1GmW+E29NmN5Gq0aRzG0S0uvQzF1QO/EplxzQmcmLdhq1/cvhyV+U29l5xfxxdp0zu3bjghr5jGu\n357Vg/i4KB76eKV19JbBfi2m3pq7Np28whJr5jFHaRYbyQPn9mLp9gzr6C2DJX5Tb81YlkabJtEM\n7mzNPOZoY45Lso7ecgR668W7RGSliKwSkSM3WX9aRNaKyHIRmSoizcsoO1JE1onIRhG5P5jBm9B1\nOK+QlPV7Oa9fO7tEg/kF6+gtX4WJX0T6Ajfj3CR9ADBKRLoBc4C+qtof5+brD/gpGw78DzgX6A1c\nKSK9gxe+CVVzVu+hoKiECwZYM4/xzzp6yxbIHn8vYIGq5qhqETAPGKOqs933APOB9n7KDgE2quqP\nqloATAYuCkbgJrTNWL6LxGYxDOrQwutQTB1mHb3+BZL4VwLDRSReRBrh3E+3Q6lpbgA+8VM2Cdju\n836HO8yYKjuUU8jXG/Zyfn9r5jHls45e/yq82bqqrhGRp4DZQDawFPjpJpgi8kegCHirOoGIyDhg\nHEBCQgIpKSlVmk9WVlaVy5r64esdhRQWK+2KdpGSku51OEex+lf3tFSle/MwHp22nEYHN9I4ynYW\nKkz8AKr6KvAqgIg8jrPnjohcB4wCzlD/F8XeydH/Dtq7w/wtYzwwHiA5OVlHjBgR0AqUlpKSQlXL\nmvphwoSFdGiZxfUXnoZI3foRW/2rmxKOzWTUf79mQU5rHj27r9fheC7Qo3rauM8dgTHA2yIyErgX\nuFBVc8oougjoLiJdRCQKuAKYVv2wTag6kF3Atxv3cX6/xDqX9E3dZR29Rwv0OP4pIrIamA7crqoZ\nwHNAE2COiCwVkRcBRCRRRGYBuJ2/dwCfAWuA91R1VbBXwoSOT1fuprhE7aQtU2nW0fuzQJt6hvsZ\n1q2MadNwOoCPvJ8FzKpqgMb4mrkijS6t4uiT2NTrUEw9c6Sj9573l/FB6g4uG1z6GJXQYWfumnpj\n7+F8vt+0n1H921kzj6mSMcclkdzJzui1xG/qjU9X7qJEYVR/u9OWqRoR4ZGL7IxeS/ym3pi+fBfd\n2jSmR0Jjr0Mx9Zh19FriN/XEnsw8Fm05YM08JihCvaPXEr+pF2at2IVaM48JklA/o9cSv6kXZizf\nRc+2TejWxpp5THCEckevJX5T56Vl5JK69SAXDLC9fRM8odzRa4nf1Hkzl+8C4Px+dtKWCa5Q7ei1\nxG/qvBnL0+ib1JTOreK8DsU0QKHY0WuJ39Rp2w/ksGzHIevUNTUmFDt6LfGbOm2GNfOYWnCko/fx\nT9aQfjjP63BqnCV+U6fNWJ7GwA7N6dCykdehmAZMRHjykn7kFBTz4Icr8X+V+YbDEr+pszbvy2ZV\nWqZdidPUim5tmnDvOcfy+Zo9TPnB721DGgxL/KbOmrEsDYDzLfGbWnL9SV0Y0rklf522irSMXK/D\nqTGW+E2dNXPFLpI7taBds1ivQzEhIjxMePrS/hSVKPdNWd5gm3ws8Zs6aWP6YdbuPmzNPKbWdYqP\n48Hze/H1hn28tWCb1+HUCEv8pk6avmwXInCeHc1jPHDV0I4M796Kx2etYdv+su4sW38Fes/du0Rk\npYisEpG73WGXuu9LRCS5nLJbRGSFe3vGxcEK3DRcqsqM5WkM7dKSNk1jvA7HhCAR4alL+hMuwu/f\nX9bgTuzqEB7yAAAbEklEQVSqMPGLSF/gZmAIMAAYJSLdgJU4N17/KoDlnKaqA1W1zA2EMUes3X2Y\nTXuz7aQt46nE5rH8+YLeLNxygAnfbvY6nKAKZI+/F7BAVXPcm6fPA8ao6hpVXVez4ZlQNHP5LsIE\nRvZt63UoJsT96vj2nNmrDX//bB0b07O8DidoArnZ+krgMRGJB3JxbqRemSYbBWaLiAIvqep4fxOJ\nyDhgHEBCQgIpKSmVWMTPsrKyqlzWeE9VeX9BLr1ahrFy8fdeh1NpVv8anlFtS/h+YwnjXv2aPw6N\nITys/t8IqMLEr6prROQpYDaQDSwFiiuxjJNVdaeItAHmiMhaVf1F85C7QRgPkJycrCNGjKjEIn6W\nkpJCVcsa763ceYg9n33Db0f2ZcSQjl6HU2lW/xqm8LZp3PnOEtZKB24f0c3rcKotoM5dVX1VVY9X\n1VOAg0DAF69W1Z3uczowFaevwBi/pi9PIyJMrJnH1CkXDEjk/P7teObz9axOy/Q6nGoL9KieNu5z\nR5wO3bcDLBcnIk2OvAbOxmk6MuYXVJWZy3dxcvdWNG8U5XU4xhzl0Yv60iw2knveX0ZBUYnX4VRL\noMfxTxGR1cB04HZVzRCRi0VkB3ACMFNEPgMQkUQRmeWWSwC+EZFlwEJgpqp+GuR1MA3Esh2H2HEw\n147mMXVSy7gonhjTnzW7MvnvFxu8DqdaAuncRVWH+xk2FafppvTwNJwOYFT1R5xDQI2p0IxlaUSF\nh3FW7wSvQzHGr7N6J3DJce15PmUTZ/ZKYECH5l6HVCV25q6pE5Ztz+DthdsYcWxrmsVGeh2OMWX6\n8wW9ad04mnveX0ZeYWWOc6k7LPEbz21Mz+K61xYS3ziKv43u63U4xpSrWWwkf/9VfzamZ/HP2fXz\nVCZL/MZTaRm5XPPqAsLDwph4w1C7RIOpF07p0ZqxQzvyyjebWbj5gNfhVJolfuOZg9kFXDNhIYfz\ninjjhsF2M3VTrzx4Xi/at4jl9+8vIzu/yOtwKsUSv/FEdn4R172+iG0Hcnj52mT6JDbzOiRjKiUu\nOoKnfzWA7QdzePKTtV6HUymW+E2tKygq4dZJqazYkcFzVw5iWNd4r0MypkqGdY3n+hO7MHH+Vr7Z\nsM/rcAJmid/UquIS5XfvLeXrDft48pL+nN3HztA19du9I4+la+s47v1gGZl5hV6HExBL/KbWqCp/\nmbaKGct38cC5PbksuYPXIRlTbTGR4fzz0gHszszjr9NWex1OQCzxm1rzzOcbmDh/K7ec0pVbTj3G\n63CMCZpBHVtw+2ndmPLDDt5btN3rcCpkid/Uije+28J/5m7g0uPbc/+5Pb0Ox5igu+uM7px4TDx/\n+nglK3ce8jqcclniNzXu46U7+cv0VZzVO4EnxvRDpP5fz9yY0iLCw3j2ykHEx0Vx66RUMnIKvA6p\nTJb4TY2at34v97y3jMGdW/LfKwcREW5VzjRcrRpH8/zY49iTmcddk5fW2Xv12q/Q1Jgfth3k1omp\n9EhowivXJhMTGe51SMbUuEEdW/DnC/owb/1e/jO3bl7F0xK/qREb9hzmhtcX0aZpNG/cMISmMXbh\nNRM6rhrakTHHJfHsFxv4cm261+H8giV+E3Q7DuZw9asLiQx3rr/Tukm01yEZU6tEhMdG96Nn26bc\nNXkJ2/bneB3SUSzxm6Dan5XPNa8uJLugiDdvGELH+EZeh2SMJ2KjwnnxquNQ4NZJqXXqEs6W+E3Q\nZOUXcf3ri9iZkcuE6wbTq11Tr0MyxlOd4uN45vKBrN6VyZ8+Wolq3ejsDfSeu3eJyEoRWSUid7vD\nLnXfl4hIcjllR4rIOhHZKCL3BytwU7eUlCh3vP0Dq9IyeeGq4xjcuaXXIRlTJ5zRK4H/O70bH6Tu\n4J2FdePkrgoTv4j0BW4GhuDcRnGUiHTDuWn6GOCrcsqGA/8DzgV6A1eKSO8gxG3qmLcWbCVl3V7+\ncmEfTu9pt040xtddZ/ZgePdW/GXaKpZtz/A6nID2+HsBC1Q1R1WLgHnAGFVdo6oV3X5mCLBRVX9U\n1QJgMnBR9UI2dc2Wfdk8Pmstp/ZozVVDO3odjjF1TniY8OwVg2jdJJrfTErlQLa3J3cFcrP1lcBj\nIhIP5OLcSH1xgPNPAnz/2+wAhvqbUETGAeMAEhISSElJCXARR8vKyqpyWVN5Jao8sSAP0RJGJ2Yx\nb948r0PylNU/U56beimPLcjj6ufnck9yDGEencVeYeJX1TUi8hQwG8gGlgJB755W1fHAeIDk5GQd\nMWJEleaTkpJCVcuaynv5qx/ZkLGGf18+gIsHtfc6HM9Z/TPlGQHEttvG/R+uILWgHX84x5vrVgXU\nuauqr6rq8ap6CnAQWB/g/HcCvtfebe8OMw3Ahj2HeXr2Os7pk8DogUleh2NMvXDFkI5cntyB/325\niTmr93gSQ6BH9bRxnzvidOi+HeD8FwHdRaSLiEQBVwDTqhKoqVuKiku45/1lNI6O4LGL7cJrxlTG\nXy/qQ7+kZvzu3aVs3pdd68sP9Dj+KSKyGpgO3K6qGSJysYjsAE4AZorIZwAikigiswDczuA7gM+A\nNcB7qroq6Gthat0LKZtYvuMQfxvdl1aN7cxcYyojJjKc58ceR3i48JtJqeQU1O7N2gNt6hmuqr1V\ndYCqznWHTVXV9qoaraoJqnqOOzxNVc/zKTtLVXuo6jGq+ljNrIapTavSDvGfuRu4cEAi5/Vr53U4\nxtRLHVo24pnLB7Juz2H+OLV2T+6yM3dNpeQXFXPPe8toERfFIxf18TocY+q1Ece24bdn9mDqkp1M\nnL+11pZrid9UyrNzN7B292GeuqQfzRtFeR2OMfXeHad14/SebXh0xmpStx6slWVa4jcBW7LtIC+k\nbOKy5PZ2dq4xQRIWJvz7soG0axbL7W/9QHZ+zbf3B3IClzHkFRZzz/vLaNcslodG2VU3jAmmZo0i\neeGq49iwJ4u46JpPy5b4TUCe/mwdP+7N5q2bhtLEbqpiTND1SWxGn8RmtbIsa+oxFZr/434mfLuZ\na07oxEndWnkdjjGmmizxm3Jl5xfxhw+W0bFlI+4/15vTy40xwWVNPaZcj89aw46Dubx/ywk0irLq\nYkxDYHv8pkxfrd/LWwu2cfPwriTbjVWMaTAs8Ru/DuUWct+U5XRr05jfndXD63CMMUFk/92NX49M\nX0364XymXn08MZHhXodjjAki2+M3vzBn9R6m/LCD20ccQ//2zb0OxxgTZJb4zVEOZBfwwIcr6N2u\nKXec3t3rcIwxNcCaesxRHvp4JYdyC5h00xCiImy/wJiGyH7Z5ifTl6Uxc/ku7j6zBz3bNvU6HGNM\nDbHEbwBYv+cwD05dwYAOzbnllK5eh2OMqUGB3nrxLhFZKSKrRORud1hLEZkjIhvc5xZllC0WkaXu\nw267WAelH87j+tcWERMZzv9+PYiIcNsfMKYhq/AXLiJ9gZuBIcAAYJSIdAPuB+aqandgrvven1xV\nHeg+LgxS3CZIcgqKuPH1xRzILmDCtYNp36KR1yEZY2pYILt2vYAFqprj3kN3Hs4N1y8C3nCneQMY\nXTMhmppSXKL83ztLWJV2iP9eOYh+7WvnyoDGGG8FkvhXAsNFJF5EGgHnAR2ABFXd5U6zGyjrzhwx\nIrJYROaLiG0c6pBHZ6zm8zXp/OXCPpzZ226sYkyoqPBwTlVdIyJPAbOBbGApUFxqGhWRsu4U3ElV\nd4pIV+ALEVmhqptKTyQi44BxAAkJCaSkpFRuTVxZWVlVLhtKZm8p5O21BZzTOYKO+VtISdnidUgN\ngtU/Ux9IZe/sLiKPAzuAu4ARqrpLRNoBKap6bAVlXwdmqOoH5U2XnJysixcvrlRcR6SkpDBixIgq\nlQ0Vn63aza2TUjmnd1ueH3scYWHidUgNhtU/4xURSVXV5ECmDfSonjbuc0ec9v23gWnAte4k1wIf\n+ynXQkSi3detgJOA1YEs09SMpdszuGvyEvq3b86/Lx9oSd+YEBTombtTRCQeKARuV9UMEXkSeE9E\nbgS2ApcBiEgycKuq3oTTMfySiJTgbGSeVFVL/B7ZfiCHm95YROsm0bxyTTKxUXbxNWNCUUCJX1WH\n+xm2HzjDz/DFwE3u6++AftWM0QTBoZxCrnttIQVFJUwedwKtm0R7HZIxxiN2rZ4QkF9UzC2TFrPt\nQA4TbxxKtzaNvQ7JGOMhS/wNnKrywJQVzP/xAM9cPpBhXeO9DskY4zE7N7+Be+bzDXy4ZCf3nNWD\n0YOSvA7HGFMHWOJvwD5I3cF/5m7g0uPbc8fp3bwOxxhTR1jib6C+3biP+6cs56Ru8Tw+ph8idtim\nMcZhib8BWr/nMLdOSqVr6zieH3s8kXa1TWOMD8sIPn7cm8XhvEKvw6gW30ssT7huMM1iI70OyRhT\nx1jidx3KKWTUf7/hken19/wyu8SyMSYQlvhdU37YQU5BMbNW7CK3oLjiAnVMflExd7xtl1g2xlTM\nEj/Ose5vLdhKi0aRZBcUM2fNHq9DqpS8wmLGvZnKF2vTeXR0X7vEsjGmXJb4ge9/3M+mvdk8cF4v\n2jWL4aMlO70OKWDZ+UVc/9oivtqwl6cu6cfYoZ28DskYU8dZ4gcmzd9Ks9hILhyQyIUDE5m3fi/7\ns/K9DqtCmXmFXDNhIQu3OGflXj64o9chGWPqgZBP/OmZecxetYdLj29PTGQ4Fw9KorhEmbF8V8WF\nPXQwu4CxLy9g+Y4MnrtyEBcNtLNyjTGBCfnEP3nRdopKlLHDnCaSnm2b0rNtEz5aWnebe/YezufK\nl+ezbs9hXrr6eM7t187rkIwx9UhIJ/6i4hLeWbiNk7u1okuruJ+GXzwoiSXbMtiyL9vD6PzbfSiP\ny8d/z9b9Obx23WBO72kducaYygnpxP/F2nR2HcrjqmFHd4heODAREercXv/2Azlc9tL3pGfm8+aN\nQzipWyuvQzLG1EMhnfgnLdhG26YxnNmrzVHD2zWLZViXeD5aspPK3pO4pmzel83lL33PodxC3rpp\nKIM7t/Q6JGNMPRXoPXfvEpGVIrJKRO52h7UUkTkissF9blFG2WvdaTaIyLX+pvHC1v3ZfLV+L1cM\n6UCEn2vZXDwoiS37c1i6PcOD6I62fs9hLnvpe/KKSnjn5mEM6NDc65CMMfVYhYlfRPoCNwNDgAHA\nKBHpBtwPzFXV7sBc933psi2Bh4GhbvmHy9pA1La3F2wjPEy4ooxDIEf2a0tURBgfL02r5ciOtnLn\nIa4YPx8B3h03jN6JTT2NxxhT/wWyx98LWKCqOapaBMwDxgAXAW+407wBjPZT9hxgjqoeUNWDwBxg\nZPXDrp68wmLeW7yds3ol0LZZjN9pmsZEclavBKYvS6OwuKSWI3Qs2XaQX788n9jIcN675QS6JzTx\nJA5jTMMSyK0XVwKPiUg8kAucBywGElT1yMHuuwF/h5ckAdt93u9wh/2CiIwDxgEkJCSQkpISSPy/\nkJWVVWHZ79KKOJhTSL/YjHKn7RZZxMzsAp7/8AsGtK7du1SuO1DMv1PzaBot/HZAJFtWLmJLrUZg\nqiKQ+meM1yrMZqq6RkSeAmYD2cBSoLjUNCoi1eoFVdXxwHiA5ORkHTFiRJXmk5KSQkVl//vCd3Rp\nFcFvxpxKWFjZNyg5saiEN9Z+zqaieO4aMahK8VTF1xv28u+5i0lqGcfbNw8joan/fyWm7gmk/hnj\ntYA6d1X1VVU9XlVPAQ4C64E9ItIOwH1O91N0J9DB5317d5hn1uzKJHXrQcYO7Vhu0geIigjj/H7t\nmL16N1n5RbUS3+er93Dj64vpHB/Hu7ecYEnfGBN0gR7V08Z97ojTvv82MA04cpTOtcDHfop+Bpwt\nIi3cTt2z3WGemTR/K9ERYfzq+PYBTX/xoCTyCkuYvWp3DUcGn6zYxa2TUunVrgmTxw2jVePoGl+m\nMSb0BHoc/xQRWQ1MB25X1QzgSeAsEdkAnOm+R0SSReQVAFU9ADwKLHIfj7jDPHE4r5CpS3Yyqn8i\nzRtFBVTm+E4taN8ilqk1fMXOnRm53P3uUgZ0aM6km4YGHJ8xxlRWQD2Wqjrcz7D9wBl+hi8GbvJ5\nPwGYUI0Yg+ajJTvJKSjm6hMCv3SxiHDxoCT+9+VG0jPzaFNDTS9PzFqDCDx75SCaxNjtEo0xNSdk\nztxVVSbN30bfpKYMqOTdqS4amESJwrRlNXNM/8LNB5ixfBe3nnoMSc1ja2QZxhhzRMgk/sVbD7Ju\nz2GuGtoJkfI7dUvr1qYx/ZKa1ci1e4pLlL9OX0VisxhuOeWYoM/fGGNKC5nEP2n+VprERHDhwMQq\nlR89KImVOzPZmH44qHG9v3g7q9IyeeC8XsRGhQd13sYY409IJP79Wfl8smI3lxzXnkZRVTsR64IB\n7QgT+GhJ8Jp7MvMKefqzdQzu3IJR/e2a+saY2hESif+9xTsoKC5h7NCq35qwTZMYTu7emo+W7qSk\nJDhX7Pzv3A0cyCng4Qv6VLr5yRhjqqrBJ/6SEuXthVsZ2qVlta91M3pgIjsO5pK67WC149q0N4vX\nvt3C5ckd6JtUuc5mY4ypjgaf+Odt2Mv2A7m/uNlKVZzTpy2xkeFBOab/sZlriI0M556zj632vIwx\npjIafOJ/a/5WWjWO5pw+bas9r7joCM7uk8DM5bsoKKr6FTtT1qXzxdp07jyjG62b2Nm5xpja1aAT\n/86MXL5Ym87lg9sTFRGcVR09KIlDuYWkrPN3aaKKFRaX8OiM1XRpFcd1J3YJSkzGGFMZDTrxv7Ng\nGwBXDql6p25pw7u1Ij4uqsrH9E/8fiub9mbzp/N7BW1jZIwxldFgM09BUQmTF23n9J5taN+iUdDm\nGxEexgUDEvl8TTqHcgsrVXZ/Vj7//nw9p/Rozek921RcwBhjakCDTfyzV+9mX1Y+Y4PQqVva6EFJ\nFBSV8OnKXRVP7ONfc9aTU1DMn0f1ssM3jTGeabCJf+L3W+nQMpZTu7cO+rwHtG9Gl1ZxlTq6Z3Va\nJu8s3MY1J3SiWxu7haIxxjsNMvFv2HOYBZsP8OshnSq82UpViAijByaxYPMB0jJyK5xeVXlkxiqa\nxUZy9xk9gh6PMcZURoNM/G8t2EZUeBiXJQd2s5WqGD0oEQ3wip2frtzN/B8PcM/Zx9KskV1y2Rjj\nrQaX+POLlCmpOzi3X1via/AOVp3i4xjUsTkfVdDck1dYzGOz1tCzbZOgHl1kjDFV1eAS//xdRRzO\nLwrKmboVuXhQEmt3H2bNrswyp3nl6x/ZcTCXP1/Qm/AaaHYyxpjKCvSeu78VkVUislJE3hGRGBE5\nXUR+cIe9ISJ+L3spIsUistR9TAtu+EdTVb7YXsSxCU1I7tSiJhcFwPn92hERJmXu9e8+lMf/vtzE\nyD5tOfGYVjUejzHGBKLCxC8iScD/Acmq2hcIB34NvAFc4Q7bys83Xi8tV1UHuo8LgxS3X8t2HGJr\nZglXDetYK4dLxjeO5tQerfl4aZrfK3Y+9elailV58LxeNR6LMcYEKtCmnggg1t2rbwRkAwWqut4d\nPwe4pAbiq5RJ87cSE+4cZ19bRg9KYndmHvM37z9qeOrWg0xdspObh3ehY3zwTiAzxpjqqvCuJKq6\nU0T+AWwDcoHZwHvA30Uk2b25+q+ADmXMIkZEFgNFwJOq+pG/iURkHDAOICEhgZSUlEqtSG6RMm1J\nDkPaKKnzv61U2eqIKlZiwuGFWakU9HM6k0tUeXR+Hs2jhX7hu0hJ2V1r8RhvZWVlVbruGlPbKkz8\nItICuAjoAmQA7wNjgSuAf4tINM7GoLiMWXRyNx5dgS9EZIWqbio9kaqOB8YDJCcn64gRIyq9Mt37\nH2bJ4kVUpWx1nL9/GbNX7WbYScOJiQxnSuoONh9axr8uG8DI42rukFJT96SkpNR6/TOmsgJp6jkT\n2Kyqe1W1EPgQOFFVv1fV4ao6BPgKWO+vsKrudJ9/BFKAQUGJ3I9ubZrQulHtH6h08aAkDucXMXdN\nOln5RTz16VoGdmjO6IG11+RkjDGBCiRLbgOGiUgjcXpMzwDWiEgbAHeP/z7gxdIFRaSFOx4RaQWc\nBKwOVvB1xQnHxNOmSTQfLd3J819uJP1wPg9f0LtGzho2xpjqqjDxq+oC4APgB2CFW2Y88AcRWQMs\nB6ar6hcAIpIsIq+4xXsBi0VkGfAlTht/g0v84WHCRQMTSVmXzitfb2bMcUkM6ljzh5MaY0xVVNjG\nD6CqDwMPlxr8B/dRetrFwE3u6++AftWMsV64aGASL3+9mUZRYdw3sqfX4RhjTJkCSvymYn0Sm3Lh\ngESGdY0noWmM1+EYY0yZLPEHiYjw7JU11m9tjDFB0+Cu1WOMMaZ8lviNMSbEWOI3xpgQY4nfGGNC\njCV+Y4wJMZb4jTEmxFjiN8aYEGOJ3xhjQkydO4FLRC4A9onI1jImaQYcKmcWrYB9QQ+s9lS0fnV9\nedWdX2XLV2b6QKat7jRW/7xdXm3Xv8qUCdZ0ZY0P/EbjqlqnHsD4ao5f7PU61OT61/XlVXd+lS1f\nmekDmba601j983Z5tV3/KlMmWNMF4zOri00906s5vr6r7fUL9vKqO7/Klq/M9IFMG6xp6iurfzVX\nJljTVfszE3cL0mCIyGJVTfY6DhOarP6Z+qAu7vFX13ivAzAhzeqfqfMa3B6/McaY8jXEPX5jjDHl\nsMRvjDEhxhK/McaEmJBK/CISJyKLRWSU17GY0CMivUTkRRH5QER+43U8JnTVi8QvIhNEJF1EVpYa\nPlJE1onIRhG5P4BZ3Qe8VzNRmoYsGHVQVdeo6q3AZcBJNRmvMeWpF0f1iMgpQBbwpqr2dYeFA+uB\ns4AdwCLgSiAceKLULG4ABgDxQAywT1Vn1E70piEIRh1U1XQRuRD4DTBRVd+urfiN8VXnrtXjj6p+\nJSKdSw0eAmxU1R8BRGQycJGqPgH8oilHREYAcUBvIFdEZqlqSU3GbRqOYNRBdz7TgGkiMhOwxG88\nUS8SfxmSgO0+73cAQ8uaWFX/CCAi1+Hs8VvSN9VVqTro7nyMAaKBWTUamTHlqM+Jv0pU9XWvYzCh\nSVVTgBSPwzCmfnTulmEn0MHnfXt3mDG1xeqgqZfqc+JfBHQXkS4iEgVcAUzzOCYTWqwOmnqpXiR+\nEXkH+B44VkR2iMiNqloE3AF8BqwB3lPVVV7GaRouq4OmIakXh3MaY4wJnnqxx2+MMSZ4LPEbY0yI\nscRvjDEhxhK/McaEGEv8xhgTYizxG2NMiLHEb4wxIcYSvzHGhBhL/MYYE2L+H5PlHG8rbW/nAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121961b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_vals, accuracy_vals)\n",
    "plt.grid(True)\n",
    "plt.title('Test Accuracy by Regularization (1-Layer Network)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 337.635712\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 28.0%\n",
      "Minibatch loss at step 10: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.8%\n",
      "Test accuracy: 72.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 4\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step % num_batches) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 10 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs extremely well on the training data (100%) but performs poorly when it comes to the validation or test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal Case:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 488.474548\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 19.7%\n",
      "Minibatch loss at step 500: 40.677055\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 1000: 19.604404\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 1500: 18.340063\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 2000: 7.369796\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 2500: 5.660817\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 3000: 3.534811\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.3%\n",
      "Test accuracy: 87.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitting Case:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 451.130188\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 36.3%\n",
      "Minibatch loss at step 10: 11.542021\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 20: 25.716421\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 40: 0.390678\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 50: 0.093071\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 60: 0.365234\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 90: 0.642761\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.9%\n",
      "Test accuracy: 75.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 4\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step % num_batches) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 10 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model still performs extremely well on the training data (~100%) and performs slightly better when it comes to the validation or test data. The model seems to be more generalized after implementing dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes_1 = 1024\n",
    "hidden_nodes_2 = 512\n",
    "hidden_nodes_3 = 256\n",
    "hidden_nodes_4 = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nodes_1],\n",
    "                       stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_nodes_1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes_1, hidden_nodes_2],\n",
    "                       stddev=np.sqrt(2.0 / hidden_nodes_1)))\n",
    "  biases2 = tf.Variable(tf.zeros([hidden_nodes_2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes_2, hidden_nodes_3],\n",
    "                       stddev=np.sqrt(2.0 / hidden_nodes_2)))\n",
    "  biases3 = tf.Variable(tf.zeros([hidden_nodes_3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes_3, hidden_nodes_4],\n",
    "                       stddev=np.sqrt(2.0 / hidden_nodes_3)))\n",
    "  biases4 = tf.Variable(tf.zeros([hidden_nodes_4]))\n",
    "  weights5 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes_4, num_labels],\n",
    "                       stddev=np.sqrt(2.0 / hidden_nodes_4)))\n",
    "  biases5 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  lay4_train = tf.nn.relu(tf.matmul(lay3_train, weights4) + biases4)\n",
    "  logits = tf.matmul(lay4_train, weights5) + biases5\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay4_valid = tf.nn.relu(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay4_valid, weights5) + biases5)\n",
    "\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  lay4_test = tf.nn.relu(tf.matmul(lay3_test, weights4) + biases4)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay4_test, weights5) + biases5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.362457\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 28.1%\n",
      "Minibatch loss at step 1000: 0.487361\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 2000: 0.243773\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 3000: 0.347435\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 4000: 0.264907\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 5000: 0.261063\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 6000: 0.290159\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 7000: 0.332008\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 8000: 0.336070\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 9000: 0.196706\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 10000: 0.162414\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 11000: 0.120712\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 12000: 0.198203\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 13000: 0.171149\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 14000: 0.089510\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 15000: 0.058991\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 16000: 0.006470\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 17000: 0.022592\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 18000: 0.044385\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.3%\n",
      "Test accuracy: 96.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 1000 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Accuracy: 96.3%"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
